{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import EfficientNetB0  # Changed to EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the configuration file\n",
    "with open('/Users/andreshofmann/Desktop/Studies/Uol/7t/FP/fp_legonet/research/code/CNN/config_v11_data_sub_set_cnn.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Paths\n",
    "base_dir = config['paths']['base_dir']\n",
    "csv_path = config['paths']['csv_path']\n",
    "img_dir = config['paths']['img_dir']\n",
    "model_save_path = config['paths']['model_save_path']\n",
    "\n",
    "# Training parameters\n",
    "validation_split = config['training']['validation_split']\n",
    "fraction = config['training']['fraction']\n",
    "sample_fraction = config['training'].get('sample_fraction', 1.0)  # Default to 1.0 if not specified\n",
    "batch_size = config['training']['batch_size']\n",
    "epochs = config['training']['epochs']\n",
    "initial_learning_rate = config['training']['initial_learning_rate']\n",
    "learning_rate_decay = config['training']['learning_rate_decay']\n",
    "patience = config['training']['patience']\n",
    "reduce_lr_factor = config['training']['reduce_lr_factor']\n",
    "reduce_lr_patience = config['training']['reduce_lr_patience']\n",
    "min_lr = config['training']['min_lr']\n",
    "\n",
    "# Data augmentation parameters\n",
    "data_aug_params = config['data_augmentation']\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Function to extract patch number from Patch_id\n",
    "def get_patch_folder(patch_id):\n",
    "    parts = patch_id.split('_')\n",
    "    patch_num = parts[-1].split('.')[0]  # Extract patch number\n",
    "    return f'Patch_{patch_num}'\n",
    "\n",
    "# Debugging version of get_full_path\n",
    "def get_full_path(row):\n",
    "    try:\n",
    "        patch_folder = get_patch_folder(row['Patch_id'])\n",
    "        if row['label'] == 1:\n",
    "            full_path = os.path.join(img_dir, 'mel_patches', patch_folder, row['Patch_id'])\n",
    "        else:\n",
    "            full_path = os.path.join(img_dir, 'bkl_patches', patch_folder, row['Patch_id'])\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"Invalid path: {full_path}\")\n",
    "\n",
    "        return full_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Apply the function to add the full path\n",
    "data['path'] = data.apply(get_full_path, axis=1)\n",
    "\n",
    "# Check for invalid paths and remove them\n",
    "data['path_exists'] = data['path'].apply(os.path.exists)\n",
    "invalid_paths = data[~data['path_exists']]\n",
    "print(f\"Number of invalid paths: {len(invalid_paths)}\")\n",
    "if not invalid_paths.empty:\n",
    "    print(invalid_paths.head())\n",
    "data = data[data['path_exists']]\n",
    "\n",
    "# Convert label column to string\n",
    "data['label'] = data['label'].astype(str)\n",
    "\n",
    "# Sample a fraction of the data\n",
    "sampled_data = data.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(sampled_data, test_size=validation_split, stratify=sampled_data['label'], random_state=42)\n",
    "\n",
    "# Image data generator for augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=data_aug_params['rescale'],\n",
    "    rotation_range=data_aug_params['rotation_range'],\n",
    "    width_shift_range=data_aug_params['width_shift_range'],\n",
    "    height_shift_range=data_aug_params['height_shift_range'],\n",
    "    shear_range=data_aug_params['shear_range'],\n",
    "    zoom_range=data_aug_params['zoom_range'],\n",
    "    horizontal_flip=data_aug_params['horizontal_flip'],\n",
    "    vertical_flip=data_aug_params['vertical_flip'],\n",
    "    brightness_range=data_aug_params['brightness_range']\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=data_aug_params['rescale'])\n",
    "\n",
    "# Create training and validation generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_data,\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "    val_data,\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Define the HyperModel\n",
    "class EfficientNetHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(hp.Int('units', min_value=128, max_value=1024, step=128), activation='relu')(x)\n",
    "        x = Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1))(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=initial_learning_rate)),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Hyperparameter tuner\n",
    "tuner = RandomSearch(\n",
    "    EfficientNetHyperModel(),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory=base_dir,\n",
    "    project_name='skin_lesion_tuning_effnet'\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(factor=reduce_lr_factor, patience=reduce_lr_patience, min_lr=min_lr)\n",
    "\n",
    "# Run the tuner\n",
    "tuner.search(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_save_path)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define a function to pool predictions from the 16 patches to produce a final prediction for the full image\n",
    "def predict_full_image(image_id, model, data):\n",
    "    patches = data[data['image_id'] == image_id]['path'].values\n",
    "    predictions = []\n",
    "\n",
    "    for patch in patches:\n",
    "        img = load_img(patch, target_size=(64, 64))\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "        prediction = model.predict(img_array, verbose=0)\n",
    "        predictions.append(prediction[0][0])\n",
    "\n",
    "    return np.mean(predictions)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "for image_id in tqdm(val_data['image_id'].unique(), desc='Evaluating images'):\n",
    "    label = val_data[val_data['image_id'] == image_id]['label'].values[0]\n",
    "    val_labels.append(int(label))\n",
    "    final_prediction = predict_full_image(image_id, model, val_data)\n",
    "    val_predictions.append(final_prediction)\n",
    "\n",
    "# Convert predictions to binary class (0 or 1)\n",
    "val_predictions_binary = [1 if pred >= 0.5 else 0 for pred in val_predictions]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(np.array(val_predictions_binary) == np.array(val_labels))\n",
    "print('Validation accuracy:', accuracy)\n",
    "\n",
    "# Example usage for a single image prediction\n",
    "image_id = 'ISIC_0028965'\n",
    "final_prediction = predict_full_image(image_id, model, data)\n",
    "print(f'Final prediction for image {image_id}:', final_prediction)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
