{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load configuration from JSON file\n",
    "config_path = '/content/drive/MyDrive/2024/University/FP/config_v6_86_acc.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Extract paths from config\n",
    "base_dir = config['paths']['base_dir']\n",
    "csv_path = config['paths']['csv_path']\n",
    "img_dir = config['paths']['img_dir']\n",
    "model_save_path = config['paths']['model_save_path']\n",
    "\n",
    "# Check for GPU and set policy if compatible\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "else:\n",
    "    print(\"Mixed precision not enabled: Compatible GPU not detected.\")\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Function to extract patch number from Patch_id\n",
    "def get_patch_folder(patch_id):\n",
    "    parts = patch_id.split('_')\n",
    "    patch_num = parts[-1].split('.')[0]  # Extract patch number\n",
    "    return f'Patch_{patch_num}'\n",
    "\n",
    "# Debugging version of get_full_path\n",
    "def get_full_path(row):\n",
    "    try:\n",
    "        patch_folder = get_patch_folder(row['Patch_id'])\n",
    "        if row['label'] == 1:\n",
    "            full_path = os.path.join(img_dir, 'mel_patches', patch_folder, row['Patch_id'])\n",
    "        else:\n",
    "            full_path = os.path.join(img_dir, 'bkl_patches', patch_folder, row['Patch_id'])\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"Invalid path: {full_path}\")\n",
    "\n",
    "        return full_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Apply the function to add the full path\n",
    "data['path'] = data.apply(get_full_path, axis=1)\n",
    "\n",
    "# Check for invalid paths and remove them\n",
    "data['path_exists'] = data['path'].apply(os.path.exists)\n",
    "invalid_paths = data[~data['path_exists']]\n",
    "print(f\"Number of invalid paths: {len(invalid_paths)}\")\n",
    "if not invalid_paths.empty:\n",
    "    print(invalid_paths.head())\n",
    "data = data[data['path_exists']]\n",
    "\n",
    "# Convert label column to string\n",
    "data['label'] = data['label'].astype(str)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=config['training']['validation_split'], stratify=data['label'], random_state=42)\n",
    "\n",
    "# Use only a fraction of the data for training\n",
    "train_data = train_data.sample(frac=config['training']['fraction'], random_state=42)\n",
    "val_data = val_data.sample(frac=config['training']['fraction'], random_state=42)\n",
    "\n",
    "# Image data generator for augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=config['data_augmentation']['rescale'],\n",
    "    rotation_range=config['data_augmentation']['rotation_range'],\n",
    "    width_shift_range=config['data_augmentation']['width_shift_range'],\n",
    "    height_shift_range=config['data_augmentation']['height_shift_range'],\n",
    "    shear_range=config['data_augmentation']['shear_range'],\n",
    "    zoom_range=config['data_augmentation']['zoom_range'],\n",
    "    horizontal_flip=config['data_augmentation']['horizontal_flip'],\n",
    "    vertical_flip=config['data_augmentation']['vertical_flip'],\n",
    "    brightness_range=config['data_augmentation']['brightness_range']\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=config['data_augmentation']['rescale'])\n",
    "\n",
    "# Create training and validation generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_data,\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "    val_data,\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Define the pre-trained model with ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(units=config['model_architecture']['dense_layers'][0]['units'], activation=config['model_architecture']['dense_layers'][0]['activation']),\n",
    "    Dropout(rate=config['model_architecture']['dropout_rate']),\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "initial_learning_rate = config['training']['initial_learning_rate']\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(model_save_path, save_best_only=True, monitor='val_loss'),  # Monitor validation loss\n",
    "    EarlyStopping(patience=config['training']['patience'], restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=config['training']['reduce_lr_factor'], patience=config['training']['reduce_lr_patience'], min_lr=config['training']['min_lr'])  # Reduce learning rate on plateau\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=config['training']['epochs'],\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define a function to pool predictions from the 16 patches to produce a final prediction for the full image\n",
    "def predict_full_image(image_id, model, data):\n",
    "    patches = data[data['image_id'] == image_id]['path'].values\n",
    "    predictions = []\n",
    "\n",
    "    for patch in patches:\n",
    "        img = load_img(patch, target_size=(64, 64))\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "        prediction = model.predict(img_array, verbose=0)\n",
    "        predictions.append(prediction[0][0])\n",
    "\n",
    "    return np.mean(predictions)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "for image_id in tqdm(val_data['image_id'].unique(), desc='Evaluating images'):\n",
    "    label = val_data[val_data['image_id'] == image_id]['label'].values[0]\n",
    "    val_labels.append(int(label))\n",
    "    final_prediction = predict_full_image(image_id, model, val_data)\n",
    "    val_predictions.append(final_prediction)\n",
    "\n",
    "# Convert predictions to binary class (0 or 1)\n",
    "val_predictions_binary = [1 if pred >= 0.5 else 0 for pred in val_predictions]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(np.array(val_predictions_binary) == np.array(val_labels))\n",
    "print('Validation accuracy:', accuracy)\n",
    "\n",
    "# Example usage for a single image prediction\n",
    "image_id = 'ISIC_0028965'\n",
    "final_prediction = predict_full_image(image_id, model, data)\n",
    "print(f'Final prediction for image {image_id}:', final_prediction)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
