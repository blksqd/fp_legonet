{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images: (10015, 128, 96, 3)\n",
      "Shape of labels: (10015, 2)\n",
      "Generator input channels: 130\n",
      "Discriminator input channels: 5\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Configurations\n",
    "batch_size = 64\n",
    "num_channels = 3  # RGB images have 3 channels\n",
    "num_classes = 2\n",
    "image_size = (128, 96)  # Resize to save memory\n",
    "latent_dim = 128\n",
    "\n",
    "# Paths to data\n",
    "image_dir = '../Data/HAM/ham_images'  # Directory containing images\n",
    "csv_path = '../Experiments/two_class_metadata.csv'  # Path to CSV file\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract image IDs and classes\n",
    "image_ids = df['image_id'].values\n",
    "labels = df['class'].values\n",
    "\n",
    "# Encode labels to integers then to one-hot\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "one_hot_labels = to_categorical(integer_encoded, num_classes=num_classes)\n",
    "\n",
    "# Define function to get full image path\n",
    "def get_image_path(image_id):\n",
    "    return f\"{image_dir}/{image_id}.jpg\"  # Assuming images are in .jpg format\n",
    "\n",
    "# Load and process images\n",
    "def process_image(image_id):\n",
    "    full_path = get_image_path(image_id)\n",
    "    img = load_img(full_path, target_size=image_size, color_mode='rgb')  # Load as RGB\n",
    "    img_array = img_to_array(img).astype(\"float32\") / 255.0\n",
    "    return img_array\n",
    "\n",
    "# Create datasets\n",
    "all_images = np.array([process_image(image_id) for image_id in image_ids])\n",
    "all_labels = one_hot_labels\n",
    "\n",
    "# Ensure the images have 4 dimensions (batch_size, height, width, channels)\n",
    "# RGB images already have 3 channels, no need for additional dimension\n",
    "all_images = np.reshape(all_images, (-1, image_size[0], image_size[1], num_channels))\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_images, all_labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Print data shapes for confirmation\n",
    "print(f\"Shape of images: {all_images.shape}\")\n",
    "print(f\"Shape of labels: {all_labels.shape}\")\n",
    "\n",
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = num_channels + num_classes\n",
    "\n",
    "print(f\"Generator input channels: {generator_in_channels}\")\n",
    "print(f\"Discriminator input channels: {discriminator_in_channels}\")\n",
    "\n",
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((*image_size, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        layers.Dense((image_size[0] // 4) * (image_size[1] // 4) * generator_in_channels),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Reshape((image_size[0] // 4, image_size[1] // 4, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Conv2D(num_channels, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "\n",
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seed = 1337\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, one_hot_labels = data\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # Ensure one_hot_labels is float32\n",
    "        one_hot_labels = tf.cast(one_hot_labels, tf.float32)\n",
    "        \n",
    "        # Expand labels to match the shape of images for concatenation.\n",
    "        image_one_hot_labels = tf.reshape(one_hot_labels, (-1, 1, 1, num_classes))\n",
    "        image_one_hot_labels = tf.tile(image_one_hot_labels, [1, image_size[0], image_size[1], 1])\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim), seed=self.seed)\n",
    "        random_vector_labels = tf.concat([random_latent_vectors, one_hot_labels], axis=1)\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], axis=-1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], axis=-1)\n",
    "        combined_images = tf.concat([fake_image_and_labels, real_image_and_labels], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim), seed=self.seed)\n",
    "        random_vector_labels = tf.concat([random_latent_vectors, one_hot_labels], axis=1)\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], axis=-1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'discriminator': self.discriminator,\n",
    "            'generator': self.generator,\n",
    "            'latent_dim': self.latent_dim\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        discriminator = keras.models.clone_model(config['discriminator'])\n",
    "        generator = keras.models.clone_model(config['generator'])\n",
    "        latent_dim = config['latent_dim']\n",
    "        return cls(discriminator, generator, latent_dim)\n",
    "\n",
    "# Instantiate and compile the ConditionalGAN model\n",
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "# Fit the model to the dataset\n",
    "cond_gan.fit(dataset, epochs=20)\n",
    "\n",
    "# Save the model\n",
    "cond_gan.save('cond_gan_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConditionalGAN' object has no attribute 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/andreshofmann/Desktop/Studies/Uol/7t/FP/gan3.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andreshofmann/Desktop/Studies/Uol/7t/FP/gan3.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loaded_model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mload_model(\u001b[39m'\u001b[39m\u001b[39mcond_gan_model.keras\u001b[39m\u001b[39m'\u001b[39m, custom_objects\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mConditionalGAN\u001b[39m\u001b[39m'\u001b[39m: ConditionalGAN})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andreshofmann/Desktop/Studies/Uol/7t/FP/gan3.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Verify the model structure\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andreshofmann/Desktop/Studies/Uol/7t/FP/gan3.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(loaded_model\u001b[39m.\u001b[39;49msummary())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/envs/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/envs/.venv/lib/python3.12/site-packages/keras/src/utils/summary_utils.py:311\u001b[0m, in \u001b[0;36mprint_summary\u001b[0;34m(model, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m    308\u001b[0m non_trainable_count \u001b[39m=\u001b[39m count_params(model\u001b[39m.\u001b[39mnon_trainable_weights)\n\u001b[1;32m    309\u001b[0m non_trainable_memory_size \u001b[39m=\u001b[39m weight_memory_size(model\u001b[39m.\u001b[39mnon_trainable_weights)\n\u001b[0;32m--> 311\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mcompiled \u001b[39mand\u001b[39;00m model\u001b[39m.\u001b[39;49moptimizer \u001b[39mand\u001b[39;00m model\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m    312\u001b[0m     optimizer_weight_count \u001b[39m=\u001b[39m count_params(model\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mvariables)\n\u001b[1;32m    313\u001b[0m     optimizer_memory_size \u001b[39m=\u001b[39m weight_memory_size(model\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mvariables)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConditionalGAN' object has no attribute 'optimizer'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model('cond_gan_model.keras', custom_objects={'ConditionalGAN': ConditionalGAN})\n",
    "\n",
    "# Verify the model structure\n",
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
